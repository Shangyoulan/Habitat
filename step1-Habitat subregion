#Extracting features of the whole tumour
import radiomics
from radiomics import featureextractor
import pandas as pd
import os

# Defining feature extractor parameters
settings = {
    'binWidth': 25,
    'sigma': [1, 1, 1],
    'resampledPixelSpacing': None
}

# Defining the Feature Extractor
extractor = featureextractor.RadiomicsFeatureExtractor(**settings)

image_types = {
    "Original": {},
}

extractor.enableImageTypes(**image_types)
dataDir = os.getcwd()
image_files = set(os.listdir(os.path.join(dataDir, "images")))
mask_files = set(os.listdir(os.path.join(dataDir, "masks")))

# Finding matching and non-matching files
matched_files = image_files.intersection(mask_files)
unmatched_files = image_files.symmetric_difference(mask_files)

# Defining the results data frame
df = pd.DataFrame()

for matched_file in matched_files:
    imagePath = os.path.join(dataDir, "images", matched_file)
    maskPath = os.path.join(dataDir, "masks", matched_file)
    print(f"正在处理图像文件 {matched_file} 和掩膜文件 {matched_file}")
    try:
        featureVector = extractor.execute(imagePath, maskPath)
        df_add = pd.DataFrame.from_dict(featureVector.values()).T
        df_add.columns = featureVector.keys()
        df_add.insert(0, 'imageName', os.path.splitext(matched_file)[0])
        df = pd.concat([df, df_add])
    except Exception as e:
        print(f"Processing image files {matched_file} and mask files {matched_file} encounter an error. Error details: {e}")

result_file = os.path.join(dataDir, 'Features.xlsx')
df.to_excel(result_file, index=False)
print("Results have been saved to file：", result_file)

# Printing unmatched files
if unmatched_files:
    print("\nNo matches for the following filenames：")
    for file in unmatched_files:
        print(file)

#Determining the optimum number of clusters
import pandas as pd
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
import os

def get_K(features, K_num):
    lowest_bic = np.infty
    bic = []
    n_components_range = range(2, K_num + 1)
    cv_types = ["full"]
    for cv_type in cv_types:
        for n_components in n_components_range:
            gmm = GaussianMixture(
                n_components=n_components, covariance_type=cv_type, random_state=0
            )
            gmm.fit(features)
            bic.append(gmm.bic(features))
            if bic[-1] < lowest_bic:
                lowest_bic = bic[-1]
                best_gmm = gmm

    return best_gmm.n_components, best_gmm.covariance_type

def RFtable_generate(df, mode='Z-score'):
    c_name_list = df.columns.to_list()

    c_rest_list = ['firstorder', 'glcm', 'glrlm', 'glszm', 'gldm', 'ngtdm']
    rest_cname_list = [c_name for c_name in c_name_list if any(feature_type in c_name for feature_type in c_rest_list)]
    
    df_rest_origin = df[rest_cname_list]
    
    if mode == 'Z-score':
        df_rest = pd.DataFrame(data=StandardScaler().fit_transform(df_rest_origin), columns=df_rest_origin.columns)
    else:
        df_rest = df_rest_origin
    return df_rest

feature_file = r'./Features.xlsx'
EDI_dir = r'./feature clustering'

if not os.path.exists(EDI_dir):
    os.makedirs(EDI_dir)

K_num = 8   # EDI range 1-5, can be set 10 as well
Patients_K = pd.DataFrame()

df = pd.read_excel(feature_file)

# # Select all columns except 'Group', 'imageName'.
features_df = df.drop(columns=['Group', 'imageName'])
features_df_clean = RFtable_generate(features_df)
features_np = features_df_clean.values

# Get the best K value for the overall feature set
best_K, _ = get_K(features_np, K_num)
print(f'The optimal number of clusters for the overall feature set is：{best_K}')
np.random.seed(0)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

from sklearn.mixture import GaussianMixture
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA


def plot_gmm_clusters(features, n_components, title):
    # Fit GMM
    gmm = GaussianMixture(n_components=n_components, random_state=0).fit(features)
    labels = gmm.predict(features)

    # Apply PCA
    pca = PCA(n_components=2)
    features_pca = pca.fit_transform(features)

    # Calculate centers after PCA (to avoid dimensionality issues)
    centers_pca = np.zeros((n_components, 2))
    for i in range(n_components):
        cluster_points = features_pca[labels == i]
        centers_pca[i] = np.mean(cluster_points, axis=0)

    # Plot clusters and radiating lines
    plt.figure(figsize=(8, 6))
    for i, center in enumerate(centers_pca):
        cluster_points = features_pca[labels == i]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i+1}')
        plt.scatter(center[0], center[1], marker='*', s=150, c='r')
        for point in cluster_points:
            plt.plot([center[0], point[0]], [center[1], point[1]], c='black', linewidth=0.5)

    plt.title(title)
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.legend()
    plt.show()

# Using the dataset features_np and the number of different clusters
for k in range(2, 9):  
    plot_gmm_clusters(features_np, k, f'GMM Clustering with {k} Components')
output_file = os.path.join(EDI_dir, 'Overall optimal number of clusters.csv')
with open(output_file, 'w') as file:
    file.write(f'Overall optimal number of clusters,{best_K}')

#Generation of habitat clustering heat maps
from radiomics import featureextractor
import os
import SimpleITK as sitk
import matplotlib.pyplot as plt
from scipy import ndimage
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from radiomics import featureextractor
import os
import SimpleITK as sitk
import matplotlib.pyplot as plt
from scipy import ndimage
from concurrent.futures import ThreadPoolExecutor
import numpy as np

def process_single_file(image_file, mask_file, odir, extractor, flag_plot, flag_retest):
    try:
        print("-----------------")
        print("Processing image:", image_file)
        print("Processing mask:", mask_file)

        image = sitk.ReadImage(image_file)
        mask = sitk.ReadImage(mask_file)

        mask_arr = sitk.GetArrayFromImage(mask)

        structure_element = np.array([[[1]]])  

        erosion = ndimage.binary_erosion(mask_arr, structure=structure_element, iterations=1)
        erosion_mask = sitk.GetImageFromArray(erosion.astype(mask_arr.dtype))
        erosion_mask.CopyInformation(mask)

        patid = os.path.basename(image_file).split('-')[0]
        eroded_file = os.path.join(odir, f"Cropped_{patid}-{'retest-' if flag_retest else ''}img.nii.gz")
        sitk.WriteImage(erosion_mask, eroded_file)

        voxel_result = extractor.execute(image_file, eroded_file, voxelBased=True)

        for key, val in voxel_result.items():
            if isinstance(val, sitk.Image):
                # Resample the extracted feature image to match the original image's dimensions and spacing
                resampler = sitk.ResampleImageFilter()
                resampler.SetOutputSpacing(image.GetSpacing())
                resampler.SetSize(image.GetSize())
                resampler.SetOutputDirection(image.GetDirection())
                resampler.SetOutputOrigin(image.GetOrigin())
                resampler.SetTransform(sitk.Transform())
                resampled_val = resampler.Execute(val)
                
                if flag_plot:
                    parameter_map = sitk.GetArrayFromImage(resampled_val)
                    plt.figure()
                    plt.imshow(parameter_map[int(parameter_map.shape[0] / 2), :, :])
                    plt.title(key)

                featuremap_file = os.path.join(odir, f"{'_'.join([patid, 'test', key]) if flag_retest else '_'.join([patid,'test', key])}.nii.gz")
                sitk.WriteImage(resampled_val, featuremap_file)
            else:
                # Diagnostic feature
                print(f"{key}: {val}")

        if not voxel_result:
            print("No features extracted!")
    except Exception as e:
        print("An error occurred:", str(e))

def process_image(idir, sets, flag_plot=False, flag_retest=False):
    odir = f"{idir}_featuremap_{sets}"
    if not os.path.exists(odir):
        os.makedirs(odir)

    # Initialize extractor with your settings
    settings = {
        'binWidth': 25,
        'sigma': [1, 1, 1],
        'resampledPixelSpacing': None
    }
    extractor = featureextractor.RadiomicsFeatureExtractor(**settings)
    
    # Disable all features
    extractor.disableAllFeatures()
    
    # Enable only the feature you need
    extractor.enableFeaturesByName(firstorder=['Energy'])
    #extractor.enableImageTypes(Original={})

    tasks = []

    for root, dirs, files in os.walk(idir):
        for file in files:
            if flag_retest:
                if not file.endswith(".nii.gz"):
                    continue
            else:
                if not file.endswith(".nii.gz"):
                    continue

            image_file = os.path.join(idir, "images", file)
            mask_file = os.path.join(idir, "masks", file)
            tasks.append((image_file, mask_file, odir, extractor, flag_plot, flag_retest))

    with ThreadPoolExecutor(max_workers=4) as executor:
        for task in tasks:
            executor.submit(process_single_file, *task)

if __name__ == "__main__":
    idir = r"./"
    settings = ["R1B12"]
    flag_plot = False
    flag_retest = False

    print("*************************************")
    print("Compute 3D features using PyRadiomics extractor")
    print("*************************************")

    for sets in settings:
        process_image(idir, sets, flag_plot, flag_retest)

#Generate a clustering diagram
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np
import SimpleITK as sitk
import os
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

os.environ["OPENBLAS_NUM_THREADS"] = "1"

def compute_habitats(evaluate_dir, patientid):
    odir = evaluate_dir + '_habitats'
    if not os.path.exists(odir):
        os.makedirs(odir)
    
    sample_X = np.array([[0]])  
    Knum = determine_optimal_clusters(sample_X)

    for p in patientid:
        print(f"Processing patient: {p}")
        db_test = pd.DataFrame()

        for root, dirs, files in os.walk(evaluate_dir):
            for file in files:
                if p+'_test_original' in file:
                    print(f"Processing file: {file}")
                    feature = file.split('_test_original_')[1].split('.nii.gz')[0]

                    if not feature == 'glcm_MCC':
                        test_file = os.path.join(evaluate_dir, file)
                        test_raw = sitk.ReadImage(test_file)
                        test_arr = sitk.GetArrayFromImage(test_raw)

                        X = test_arr.flatten()
                        robu_scaler = MinMaxScaler().fit(X.reshape(-1, 1))
                        X_scaled = robu_scaler.transform(X.reshape(-1, 1))
                        X_arr = X_scaled.reshape(np.shape(test_arr))

                        if feature == 'ngtdm_Strength':
                            nanrm_test = np.isnan(X_arr)
                            test_Nanrow = np.isnan(nanrm_test)
                            print(f"test_Nanrow size: {np.sum(~test_Nanrow)}")

                            test = X_arr[~test_Nanrow]

                            df_test = pd.DataFrame(test, columns=[feature])
                            db_test = pd.concat([db_test, df_test], axis=1)

        X = db_test.replace(np.nan, 0)
        pca = PCA(n_components=X.shape[1])
        filtered_test_pca = pca.fit_transform(X)

        evar = np.sum(pca.explained_variance_ratio_) * 100
        print("Explained Variance: %s" % evar)

        habFileName = os.path.join(odir, f'{p}')
        kmeans = KMeans(n_clusters=Knum, init='k-means++', random_state=0, n_init=10)
        y_kmeans = kmeans.fit_predict(filtered_test_pca)
        cluster_arr_test = apply_cluster_labels(test_arr, test_Nanrow, y_kmeans)
        save_cluster_image(cluster_arr_test, test_raw, habFileName)

def determine_optimal_clusters(X):
    try:
        Knum = int(input("Please enter the optimum number of clusters: ")) 
    except ValueError:
        print("Invalid input, please enter an integer.")
        return determine_optimal_clusters(X)
    return Knum

def apply_cluster_labels(arr, Nanrow, labels):
    cluster_arr = np.empty(np.shape(arr))
    cluster_arr[:] = np.nan
    cluster_arr[~Nanrow] = labels
    return cluster_arr

def save_cluster_image(cluster_arr, raw, filename):
    cluster_img = sitk.GetImageFromArray(cluster_arr)
    cluster_img.CopyInformation(raw)
    sitk.WriteImage(cluster_img, filename)  
    
def get_unique_patient_ids(evaluate_dir):
    patient_ids = set()
    
    for file in os.listdir(evaluate_dir):
        parts = file.split('_')
        if parts[1] == "test" or parts[1] == "retest":
            patient_ids.add(parts[0])
    
    return list(patient_ids)

evaluate_dir = r'./_featuremap_R1B12'
patientid = get_unique_patient_ids(evaluate_dir)
compute_habitats(evaluate_dir, patientid)


def extract_labels_from_mask(mask_path, output_directory):
    """
    Extract and save individual label masks from a given mask NIFTI file.

    Parameters:
        mask_path (str): Path to the NIFTI mask file.
        output_directory (str): Directory where the individual label masks will be saved.
    """

    # Read the mask
    mask_image = sitk.ReadImage(mask_path)

    # Convert the mask to a numpy array
    mask_array = sitk.GetArrayFromImage(mask_image)

    # Get unique labels present in the mask
    unique_labels = np.unique(mask_array)
    
    # Exclude label 0 which typically represents the background
    unique_labels = unique_labels[unique_labels != 0]

    # For each label, create a binary mask and save it
    for label in unique_labels:
        binary_mask_array = np.where(mask_array == label, 1, 0).astype(np.uint8)
        
        # Convert binary numpy array back to SimpleITK Image
        binary_mask_image = sitk.GetImageFromArray(binary_mask_array)
        binary_mask_image.CopyInformation(mask_image)

        # Create a filename based on original mask filename and label value
        base_filename = os.path.basename(mask_path)
        filename_without_extension, extension = os.path.splitext(base_filename)
        if extension == ".gz":
            filename_without_extension, _ = os.path.splitext(filename_without_extension)
            extension = ".nii.gz"

        output_filename = f"{filename_without_extension}_label_{label}{extension}"
        output_path = os.path.join(output_directory, output_filename)

        # Save the binary mask image
        sitk.WriteImage(binary_mask_image, output_path)

    print(f"Processed {len(unique_labels)} labels from {mask_path}")

if __name__ == "__main__":
    input_directory = r"./_featuremap_R1B12_habitats"
    output_directory = r"./_featuremap_R1B12_habitats_分"

    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Iterate over all .nii.gz files in the input directory
    for mask_file in os.listdir(input_directory):
        if mask_file.endswith(".nii.gz"):
            mask_path = os.path.join(input_directory, mask_file)
            extract_labels_from_mask(mask_path, output_directory)
