#读取数据
import os
import pandas as pd
from IPython.display import display
from sys import exit 
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
from onekey_algo import OnekeyDS as okds
from onekey_algo import get_param_in_cwd
os.makedirs('img', exist_ok=True)
os.makedirs('results', exist_ok=True)
os.makedirs('features', exist_ok=True)
# 设置任务Task前缀
task_type = 'Habitat_'
# 设置数据目录
mydir = r''
# 对应的标签文件
group_info = 'group'
labelf = ''
# 读取标签数据列名
labels = ['label']

#images和masks匹配
from pathlib import Path
from onekey_algo.custom.components.Radiology import diagnose_3d_image_mask_settings, get_image_mask_from_dir
# 生成images和masks对，一对一的关系。也可以自定义替换。
images, masks = get_image_mask_from_dir(mydir, images=f'images', masks=f'masks')
print(f'获取到{len(images)}个样本。')

#提取特征
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
import concurrent.futures
import warnings
import pandas as pd
import numpy as np
from onekey_algo.custom.components.Radiology import ConventionalRadiomics
from onekey_algo.custom.components.habitat import habitat_feature_fusion
warnings.filterwarnings("ignore")
rad_data = None
habitats = get_param_in_cwd('habitats')
rad_ = []
param_file = r'H:/Habitat/ALL_habitats/exampleCT.yaml'
for habitat in habitats:
    if os.path.exists(f'features/rad_features_h{habitat}.csv'):
        rad_data_ = pd.read_csv(f'features/rad_features_h{habitat}.csv', header=0)
    else:
        images, masks = get_image_mask_from_dir(mydir,  images=f'images', masks=f'masks')
        radiomics = ConventionalRadiomics(param_file, correctMask=True)
        radiomics.extract(images, masks, workers=1, labels=habitat)
        rad_data_ = radiomics.get_label_data_frame(label=habitat)
        rad_data_.columns = [f"{c.replace('-', '_')}_h{habitat}" if c != 'ID' else 'ID' for c in rad_data_.columns]
        os.makedirs('features', exist_ok=True)
        rad_data_.to_csv(f'features/rad_features_h{habitat}.csv', header=True, index=False)
    rad_.append(rad_data_)
rad_data = habitat_feature_fusion(*rad_, mode=get_param_in_cwd('fusion_mode'))
rad_data.to_csv('rad_features.csv', index=False)

#通过计算每一个亚区体积与每一个亚区的特征之间的相关性，去除相关系数大于0.9的特征
import pandas as pd
volumef = r'volume_invasive.xlsx'
sheet1_name='Sheet1'
sheet2_name='Sheet2'
sheet3_name='Sheet3'
sheet4_name='Sheet4'
volume1 = pd.read_excel(volumef,sheet_name=sheet1_name)
volume2 = pd.read_excel(volumef,sheet_name=sheet2_name)
volume3 = pd.read_excel(volumef,sheet_name=sheet3_name)
volume4 = pd.read_excel(volumef,sheet_name=sheet4_name)
volume1=volume1.iloc[:,:]
volume2=volume2.iloc[:,:]
volume3=volume3.iloc[:,:]
volume4=volume4.iloc[:,:]

correlation_matrix1 = volume1.corr()
highly_correlated_features = correlation_matrix1.index[correlation_matrix1.iloc[:, 0].abs() > 0.75]
print(f"Number of features highly correlated with the first column: {len(highly_correlated_features)}")
for feature in highly_correlated_features:
    correlation_value = correlation_matrix1.loc[feature, correlation_matrix1.columns[0]]
    print(f"{feature}: {correlation_value}")
filtered_volume1 = volume1.drop(columns=highly_correlated_features)
filtered_volume1.to_csv('corr_feature1.csv', header=True, index=False)

correlation_matrix2 = volume2.corr()
highly_correlated_features2 = correlation_matrix2.index[correlation_matrix2.iloc[:, 0].abs() > 0.75]
print(f"Number of features highly correlated with the first column: {len(highly_correlated_features2)}")
for feature in highly_correlated_features2:
    correlation_value = correlation_matrix2.loc[feature, correlation_matrix2.columns[0]]
    print(f"{feature}: {correlation_value}")
filtered_volume2 = volume2.drop(columns=highly_correlated_features2)
filtered_volume2.to_csv('corr_feature2.csv', header=True, index=False)

correlation_matrix3 = volume3.corr()
highly_correlated_features3 = correlation_matrix3.index[correlation_matrix3.iloc[:, 0].abs() > 0.75]
print(f"Number of features highly correlated with the first column: {len(highly_correlated_features3)}")
for feature in highly_correlated_features3:
    correlation_value = correlation_matrix3.loc[feature, correlation_matrix3.columns[0]]
    print(f"{feature}: {correlation_value}")
filtered_volume3 = volume3.drop(columns=highly_correlated_features3)
filtered_volume3.to_csv('corr_feature3.csv', header=True, index=False)

correlation_matrix4 = volume4.corr()
highly_correlated_features4 = correlation_matrix4.index[correlation_matrix4.iloc[:, 0].abs() > 0.75]
print(f"Number of features highly correlated with the first column: {len(highly_correlated_features4)}")
for feature in highly_correlated_features4:
    correlation_value = correlation_matrix4.loc[feature, correlation_matrix4.columns[0]]
    print(f"{feature}: {correlation_value}")
filtered_volume4 = volume4.drop(columns=highly_correlated_features4)
filtered_volume4.to_csv('corr_feature4.csv', header=True, index=False)

#标注数据
group_info = 'group'
labels = ['label']
labelf = r'label_invasive.csv'
label_data['ID'] = label_data['ID'].map(lambda x: f"{x}.nii.gz" if not (f"{x}".endswith('.nii.gz') or  f"{x}".endswith('.nii')) else x)
label_data = label_data[['ID', group_info] + labels]
label_data

#特征拼接，将标注数据label_data与rad_data进行合并，得到训练数据。
from onekey_algo.custom.utils import print_join_info
print_join_info(rad_data, label_data)
combined_data = pd.merge(rad_data, label_data, on=['ID'], how='inner')
ids = combined_data['ID']
combined_data = combined_data.drop(['ID'], axis=1)
display(combined_data[labels].value_counts())
display(combined_data[group_info].value_counts())

#正则化
from onekey_algo.custom.components.comp1 import normalize_df
data = normalize_df(combined_data, not_norm=labels, group=group_info, use_train=True, verbose=True)
data = data.dropna(axis=1)

#统计检验
import seaborn as sns
from onekey_algo.custom.components.stats import clinic_stats

sub_data = data[data[group_info] == 'train']
stats = clinic_stats(sub_data, stats_columns=list(data.columns[0:-2]), label_column=labels[0], 
                     continuous_columns=list(data.columns[0:-2]))
stats

#调整p value进行筛选
pvalue = 0.05
sel_feature = list(stats[stats['pvalue'] < pvalue]['feature_name']) + labels + [group_info]
sub_data = sub_data[sel_feature]
sub_data
sub_data.to_csv('ttest_features.csv', header=True, index=False)

#计算相关系数并进行特征筛选
import math
pearson_corr = sub_data[[c for c in sub_data.columns if c not in labels]].corr('pearson')
pearson_corr
from onekey_algo.custom.components.comp1 import select_feature, select_feature_mrmr
sel_feature = select_feature(pearson_corr, threshold=0.9, topn=16, verbose=False)
sel_feature = pd.DataFrame(sel_feature)
sel_feature.to_csv('pearson_features.csv', header=True, index=False)

#mRMR特征筛选
labels = ['label']
mrmr_sel_feature_num = get_param_in_cwd('mrmr_sel_feature_num')
if mrmr_sel_feature_num is not None:
    sel_feature = select_feature_mrmr(data[sel_feature + labels], num_features=mrmr_sel_feature_num)
sel_feature += labels + [group_info]
sel_feature

#过滤特征
sel_data = data[sel_feature]
sel_data.to_csv('sel_data_20.csv', header=True, index=False)

#手动将亚区体积添加到筛选的特征之后
#将亚区体积添加到筛选的特征之后
self = r'sel_data.csv'
sel_data = pd.read_csv(self)

#构建数据
import numpy as np
import onekey_algo.custom.components as okcomp
group_info = 'group'
n_classes = 2
train_data = sel_data[(sel_data[group_info] == 'train')]
train_ids = ids[train_data.index]
train_data = train_data.reset_index()
train_data = train_data.drop('index', axis=1)
y_data = train_data[labels]
X_data = train_data.drop(labels + [group_info], axis=1)

val_data = sel_data[sel_data[group_info] == 'val']
val_ids = ids[val_data.index]
val_data = val_data.reset_index()
val_data = val_data.drop('index', axis=1)
y_val_data = val_data[labels]
X_val_data = val_data.drop(labels + [group_info], axis=1)

test_data = sel_data[sel_data[group_info] == 'test1']
test_ids = ids[test_data.index]
test_data = test_data.reset_index()
test_data = test_data.drop('index', axis=1)
y_test_data = test_data[labels]
X_test_data = test_data.drop(labels + [group_info], axis=1)

test2_data = sel_data[sel_data[group_info] == 'test2']
test2_ids = ids[test2_data.index]
test2_data = test2_data.reset_index()
test2_data = test2_data.drop('index', axis=1)
y_test2_data = test2_data[labels]
X_test2_data = test2_data.drop(labels + [group_info], axis=1)

test3_data = sel_data[sel_data[group_info] == 'test3']
test3_ids = ids[test3_data.index]
test3_data = test3_data.reset_index()
test3_data = test3_data.drop('index', axis=1)
y_test3_data = test3_data[labels]
X_test3_data = test3_data.drop(labels + [group_info], axis=1)

y_all_data = sel_data[labels]
X_all_data = sel_data.drop(labels + [group_info], axis=1)

column_names = X_data.columns
print(f"训练集样本数：{X_data.shape}, 验证集样本数：{X_val_data.shape}, 测试集1样本数：{X_test_data.shape}, 测试集2样本数：{X_test2_data.shape}, 测试集3样本数：{X_test3_data.shape}")

#LASSO
alpha = okcomp.comp1.lasso_cv_coefs(X_data, y_data, column_names=None, alpha_logmin=-4)
plt.rcParams['font.family'] = 'Times New Roman'  
plt.rcParams['font.size'] = 20
plt.savefig(f'img/{task_type}Rad_feature_lasso.svg', bbox_inches = 'tight')
plt.show()
okcomp.comp1.lasso_cv_efficiency(X_data, y_data, points=50, alpha_logmin=-2.5)
plt.savefig(f'img/{task_type}Rad_feature_mse.svg', bbox_inches = 'tight')

#特征筛选，筛选出其中coef > 0的特征
import pandas as pd
COEF_THRESHOLD = 0 # 筛选的特征阈值
scores = []
selected_features = []
for idx, label in enumerate(labels):
    feat_coef = [(feat_name, coef) for feat_name, coef in zip(column_names, model.coef_) 
                 if COEF_THRESHOLD is None or abs(coef) > COEF_THRESHOLD]
    selected_features.append([feat for feat, _ in feat_coef])
    formula = ' '.join([f"{coef:+.6f} * {feat_name}" for feat_name, coef in feat_coef])
    score = f"{label} = {model.intercept_[idx]} {'+' if formula[0] != '-' else ''} {formula}"
    scores.append(score)
    
    feat_coef = sorted(feat_coef, key=lambda x: x[1])
    feat_coef_df = pd.DataFrame(feat_coef, columns=['feature_name', 'Coefficients'])
    feat_coef_df.plot(x='feature_name', y='Coefficients', kind='barh')
    plt.savefig(f'img/{label}_{task_type}Rad_feature_weights.svg', bbox_inches = 'tight')
    
print('\n'.join(scores))
selected_features

#进一步筛选特征，使用Lasso筛选出来的Coefficients比较高的特征作为训练数据
X_data = X_data[selected_features[0]]
X_val_data = X_val_data[selected_features[0]]
X_test_data = X_test_data[selected_features[0]]
X_test2_data = X_test2_data[selected_features[0]]
X_test3_data = X_test3_data[selected_features[0]]
X_data.columns

#构建模型
import joblib
from onekey_algo.custom.components.comp1 import plot_feature_importance, smote_resample
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
# Set global font size
plt.rcParams['font.size'] = 17
plt.rcParams['axes.labelsize'] = 17
plt.rcParams['axes.titlesize'] = 20
targets = []
os.makedirs('models', exist_ok=True)
for l in labels:
    new_models = okcomp.comp1.create_clf_model(model_names)

    new_models['RandomForest'] = RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_split=2, 
                                                        min_samples_leaf=4, max_features='sqrt',random_state=100)
    model_names = list(new_models.keys())
    new_models = list(new_models.values())
    for mn, m in zip(model_names, new_models):
        X_train_smote, y_train_smote = X_train_sel, y_train_sel[l]
        if l == 'label1':
            X_train_smote, y_train_smote = smote_resample(X_train_sel, y_train_sel[l])
        m.fit(X_train_smote, y_train_smote)
        # 保存训练的模型
        joblib.dump(m, f'models/{task_type}Rad_{mn}_{l}.pkl') 
        #输出模型特征重要性
        plot_feature_importance(m, selected_features[0], save_dir='img', prefix=f'{task_type}')
        plt.show()
    targets.append(new_models)
    
#计算指标
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
from onekey_algo.custom.components.delong import calc_95_CI
from onekey_algo.custom.components.metrics import analysis_pred_binary

metric = []
pred_sel_idx = []
predictions = [[(model.predict(X_train_sel), model.predict(X_val_sel),model.predict(X_test1_sel),model.predict(X_test2_sel),model.predict(X_test3_sel))  
                for model in target] for label, target in zip(labels, targets)]
pred_scores = [[(model.predict_proba(X_train_sel), model.predict_proba(X_val_sel), model.predict_proba(X_test1_sel), model.predict_proba(X_test2_sel), model.predict_proba(X_test3_sel)) 
                for model in target] for label, target in zip(labels, targets)]
for label, prediction, scores in zip(labels, predictions, pred_scores):
    pred_sel_idx_label = []
    for mname, (train_pred, val_pred, test1_pred, test2_pred, test3_pred), (train_score,val_score, test1_score, test2_score, test3_pred) in zip(model_names, prediction, scores):
        # 计算训练集指数
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], train_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f"Train-{label}"))
        
        # 计算验证集指数
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_val_sel[label], val_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f"Val-{label}"))
        
        # 计算测试集1指标
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test1_sel[label], test1_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f'Test1-{label}'))
        
        # 计算测试集2指标
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test2_sel[label], test2_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f'Test2-{label}'))
        
        # 计算测试集3指标
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test3_sel[label], test3_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f'Test3-{label}'))    
        
        # 计算thres对应的sel idx
        pred_sel_idx_label.append(np.logical_or(test_score[:, 0] >= thres, test_score[:, 1] >= thres))
        
    pred_sel_idx.append(pred_sel_idx_label)
metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',
                                                   'Sensitivity', 'Specificity', 
                                                   'PPV', 'NPV', 'Precision', 'Recall', 'F1',
                                                   'Threshold', 'Task'])
metric

#绘制ROC曲线
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

RandomForest_data = metric[metric['model_name'] == 'RandomForest']
# 提取 Train、Val、Test 的数据
train_data = RandomForest_data[RandomForest_data['Task'].str.startswith('Train')]
val_data = RandomForest_data[RandomForest_data['Task'].str.startswith('Val')]
test1_data = RandomForest_data[RandomForest_data['Task'].str.startswith('Test1')]
test2_data = RandomForest_data[RandomForest_data['Task'].str.startswith('Test2')]
test3_data = RandomForest_data[RandomForest_data['Task'].str.startswith('Test3')]

# 绘制 ROC 曲线
plt.figure(figsize=(8, 8))

# 绘制 Train 数据的 ROC 曲线
for index, row in train_data.iterrows():
    fpr, tpr, thresholds = roc_curve(y_train_sel[row['Task'].split('-')[1]], train_score[:, 1])
    roc_auc = auc(fpr, tpr)
    ci = row['95% CI']
    #label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})\n95% CI: {ci}"
    label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})"
    plt.plot(fpr, tpr, label=label)

# 绘制 Val 数据的 ROC 曲线
for index, row in val_data.iterrows():
    fpr, tpr, thresholds = roc_curve(y_val_sel[row['Task'].split('-')[1]], val_score[:, 1])
    roc_auc = auc(fpr, tpr)
    ci = row['95% CI']
    #label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})\n95% CI: {ci}"
    label = f"{row['Task'].replace('-label', '').replace('Val', 'Validation')} (AUC = {round(roc_auc, 3)})"
    plt.plot(fpr, tpr, label=label)

# 绘制 Test1 数据的 ROC 曲线
for index, row in test_data.iterrows():
    fpr, tpr, thresholds = roc_curve(y_test1_sel[row['Task'].split('-')[1]], test1_score[:, 1])
    roc_auc = auc(fpr, tpr)
    ci = row['95% CI']
    #显示置信区间
    #label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})\n95% CI: {ci}"
    label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})"
    plt.plot(fpr, tpr, label=label)

# 绘制 Test2 数据的 ROC 曲线
for index, row in test_data.iterrows():
    fpr, tpr, thresholds = roc_curve(y_test2_sel[row['Task'].split('-')[1]], test2_score[:, 1])
    roc_auc = auc(fpr, tpr)
    ci = row['95% CI']
    #显示置信区间
    #label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})\n95% CI: {ci}"
    label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})"
    plt.plot(fpr, tpr, label=label)

# 绘制 Test3 数据的 ROC 曲线
for index, row in test_data.iterrows():
    fpr, tpr, thresholds = roc_curve(y_test3_sel[row['Task'].split('-')[1]], test3_score[:, 1])
    roc_auc = auc(fpr, tpr)
    ci = row['95% CI']
    #显示置信区间
    #label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})\n95% CI: {ci}"
    label = f"{row['Task'].replace('-label', '')} (AUC = {round(roc_auc, 3)})"
    plt.plot(fpr, tpr, label=label) 
# 随机线
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('1-Specificity',fontsize=15)
plt.ylabel('Sensitivity',fontsize=15)
plt.legend(loc='lower right', fontsize=15)  # You may adjust the font size as needed
plt.show()

#绘制决策曲线
sel_model = ['RandomForest']
import numpy as np
import matplotlib.pyplot as plt
from onekey_algo.custom.components.comp1 import plot_DCA

def find_intersection_points(x_values, y_values1, y_values2):
    intersections = []
    for i in range(1, len(x_values)):
        if (y_values1[i-1] - y_values2[i-1]) * (y_values1[i] - y_values2[i]) < 0:
            # Linear interpolation to find intersection point
            alpha = (y_values2[i] - y_values1[i]) / (y_values1[i-1] - y_values2[i-1] + y_values2[i] - y_values1[i])
            x_intersect = x_values[i-1] + alpha * (x_values[i] - x_values[i-1])
            intersections.append(x_intersect)
    return intersections 
for dataset, dataset_name in zip([y_train_sel, y_val_sel, y_test_sel], ['train', 'validation', 'test']):
    for pred_score, label in zip(pred_scores, labels):
        for sm in sel_model:
            if sm in model_names:
                sel_model_idx = model_names.index(sm)

                # Determine the dataset type
                if dataset_name == 'train':
                    data_type = 'Train'
                    scores = pred_score[sel_model_idx][0][:, 1]
                elif dataset_name == 'validation':
                    data_type = 'Val'
                    scores = pred_score[sel_model_idx][1][:, 1]
                elif dataset_name == 'test1':
                    data_type = 'Test1'
                    scores = pred_score[sel_model_idx][2][:, 1]
                elif dataset_name == 'test2':
                    data_type = 'Test2'
                    scores = pred_score[sel_model_idx][3][:, 1]
                elif dataset_name == 'test3':
                    data_type = 'Test3'
                    scores = pred_score[sel_model_idx][4][:, 1]
                # Plot DCA curve for the current dataset
                plot_DCA(scores, np.array(dataset[label]),
                          title=f'{dataset_name}')
                plt.title(f'{dataset_name}', fontsize=25)
                plt.xlabel('Threshold Probability',family = 'Times New Roman', weight='normal', fontsize=20) 
                plt.ylabel('Net Benefit', family = 'Times New Roman',weight='normal', fontsize=20)
                plt.xticks(fontsize=20)
                plt.yticks(fontsize=20)
                plt.legend(fontsize=20)
                plt.savefig(f'img/{label}_{task_type}Rad_model_{dataset_name.lower()}_{sm}_dca.svg', bbox_inches='tight')
                plt.show()

#绘制样本预测直方图
sel_model = ['RandomForest']
c_matrix = {}

for sm in sel_model:
    if sm in model_names:
        sel_model_idx = model_names.index(sm)
        for idx, label in enumerate(labels):            
            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][0], y_train_sel[label])
            plt.title(f'train', fontdict={'fontsize': 25})  # Set title font size
            plt.xticks(fontsize=20)
            plt.yticks(fontsize=20)
            plt.legend(labels=["label=0", "label=1"], loc="lower right", fontsize=20)  # Set legend font size
            plt.savefig(f'img/{label}_{task_type}Rad_train_{sm}_sample_dis.svg', bbox_inches='tight')
            plt.show()
            
            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][1], y_val_sel[label])
            plt.title(f'validation', fontdict={'fontsize': 25})
            plt.xticks(fontsize=20)
            plt.yticks(fontsize=20)
            plt.legend(labels=["label=0", "label=1"], loc="lower right", fontsize=20)
            plt.savefig(f'img/{label}_{task_type}Rad_test_{sm}_sample_dis.svg', bbox_inches='tight')
            plt.show()
            
            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][2], y_test_sel[label])
            plt.title(f'test1', fontdict={'fontsize': 25})
            plt.xticks(fontsize=20)
            plt.yticks(fontsize=20)
            plt.legend(labels=["label=0", "label=1"], loc="lower right", fontsize=20)
            plt.savefig(f'img/{label}_{task_type}Rad_test_{sm}_sample_dis.svg', bbox_inches='tight')
            plt.show()
            
            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][2], y_test2_sel[label])
            plt.title(f'test2', fontdict={'fontsize': 25})
            plt.xticks(fontsize=20)
            plt.yticks(fontsize=20)
            plt.legend(labels=["label=0", "label=1"], loc="lower right", fontsize=20)
            plt.savefig(f'img/{label}_{task_type}Rad_test_{sm}_sample_dis.svg', bbox_inches='tight')
            plt.show()
            
            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][2], y_test2_sel[label])
            plt.title(f'test3', fontdict={'fontsize': 25})
            plt.xticks(fontsize=20)
            plt.yticks(fontsize=20)
            plt.legend(labels=["label=0", "label=1"], loc="lower right", fontsize=20)
            plt.savefig(f'img/{label}_{task_type}Rad_test_{sm}_sample_dis.svg', bbox_inches='tight')
            plt.show()
#将训练好的模型导出
import joblib
trained_model = new_models[0]  # 使用索引访问列表中的元素
# 保存模型到文件
joblib.dump(trained_model, 'your_model_filename.pkl')
joblib
